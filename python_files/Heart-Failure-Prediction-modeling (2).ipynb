{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fe4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38145d39-aee3-476e-82ae-16f32780cdea",
   "metadata": {},
   "source": [
    "# **MODEL BUILDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd11196-9330-49ec-a17e-09912edfec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str) -> pd.DataFrame:\n",
    "     \"\"\"\n",
    "    This function is used to Load a dataset from the specified file path in the local device.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path of the dataset file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset is loaded as Pandas DataFrame.\n",
    "    \"\"\"\n",
    "     dataset = pd.read_csv(file_path)\n",
    "     print(f\"Heart disease Dataset is loaded from {file_path} dimension is {dataset.shape}\")\n",
    "     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ed90dd-9cbd-4a48-81f7-43502952fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart disease Dataset is loaded from C:\\Users\\SOHAM\\Git_Repositories\\DataScience_Projects\\dsp-heart-failure-prediction\\data\\heart.csv dimension is (918, 12)\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\SOHAM\\Git_Repositories\\DataScience_Projects\\dsp-heart-failure-prediction\\data\\heart.csv\"\n",
    "dataset = load_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8429e79-675e-4289-a02e-4d1001eb0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(data: pd.DataFrame, selected_features: list, target_feature: str) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    selecting the features and target variable for ml training.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to prepare features from.\n",
    "        selected_features (list): List of feature names to include.\n",
    "        target_feature (str): The name of the target feature.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.Series]: The selected features and target variable.\n",
    "    \"\"\"\n",
    "    X = data[selected_features]\n",
    "    y = data[target_feature]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c20040-5faf-49fa-9518-be3d00804a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Age', 'Sex', 'ChestPainType', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope']\n",
    "target_feature = ['HeartDisease']\n",
    "\n",
    "X, y = select_features(dataset, selected_features, target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67355611-fa81-46f2-8635-e6099cde7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data: pd.DataFrame, target_feature:pd.DataFrame , test_size: float = 0.35, random_state: int = 50) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    This function is used to Split the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to split.\n",
    "        target_feature (str): The column_name of the target feature.\n",
    "        test_size (float): The proportion of the dataset to include in the test split. Defaults to 0.35.\n",
    "        random_state (int): Seed used by the random number generator. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: The training features, testing features, training target, and testing target.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1a3352-9b57-43a7-bc82-383e1ad1338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a515a5c-5ef4-4a52-b3c8-18961c0c7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df: pd.DataFrame, Nominal_features: list, Ordinal_features: list, slope_order: list) -> pd.DataFrame:\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    ordinal_encoder = OrdinalEncoder(categories=slope_order)\n",
    "    one_hot_encoder.fit(df[Nominal_features])\n",
    "    ordinal_encoder.fit(df[Ordinal_features])\n",
    "    encoded_Nominal_features = one_hot_encoder.transform(df[Nominal_features])\n",
    "    encoded_Ordinal_features = ordinal_encoder.transform(df[Ordinal_features])                     \n",
    "    Nominal_encoded_df = pd.DataFrame(encoded_Nominal_features, columns=one_hot_encoder.get_feature_names_out(Nominal_features))\n",
    "    Ordinal_encoded_df = pd.DataFrame(encoded_Ordinal_features, columns=Ordinal_features)\n",
    "    encoded_df = pd.concat([Nominal_encoded_df, Ordinal_encoded_df], axis = 1)\n",
    "    return encoded_df, one_hot_encoder, ordinal_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bdf5e0-ce2d-4ea9-aed2-a3122beae0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nominal_features = ['ChestPainType', 'Sex', 'ExerciseAngina', 'RestingECG']\n",
    "Ordinal_features = ['ST_Slope']\n",
    "slope_order =[['Down', 'Flat', 'Up']]\n",
    "encoded_df, one_hot_encoder, ordinal_encoder = encode_categorical_features(X_train, Nominal_features, Ordinal_features, slope_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35106ced-a7fb-495c-84d5-f0831a7dcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_continuous_features(df: pd.DataFrame, continuos_features: list) -> pd.DataFrame:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[continuos_features])\n",
    "    scaled_features = scaler.transform(df[continuos_features])\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=continuos_features)\n",
    "    return scaled_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9576ebcb-b9a0-4375-995f-210953d64064",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuos_features = ['Age', 'MaxHR', 'Oldpeak']\n",
    "scaled_df, scaler = scale_continuous_features(X_train, continuos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "767a28ad-a110-4723-9bb1-88ed39cb4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_features(scaled_df: pd.DataFrame, encoded_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    concatenated = pd.concat([scaled_df, encoded_df], axis=1)\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1dc0f95-4146-4d5a-a8c8-c0b7bd9fcb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>ChestPainType_ATA</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>ExerciseAngina_Y</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>ST_Slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.475397</td>\n",
       "      <td>-0.534464</td>\n",
       "      <td>0.296557</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.794222</td>\n",
       "      <td>0.526945</td>\n",
       "      <td>-0.866898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.053611</td>\n",
       "      <td>-0.180661</td>\n",
       "      <td>-0.866898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.217428</td>\n",
       "      <td>-0.455841</td>\n",
       "      <td>0.878285</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.110206</td>\n",
       "      <td>-1.595873</td>\n",
       "      <td>1.266103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>-1.852237</td>\n",
       "      <td>1.706288</td>\n",
       "      <td>-0.866898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>-1.534832</td>\n",
       "      <td>-1.202758</td>\n",
       "      <td>-0.866898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>-1.005825</td>\n",
       "      <td>0.291076</td>\n",
       "      <td>1.847831</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.475397</td>\n",
       "      <td>-1.045513</td>\n",
       "      <td>1.072194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2.379825</td>\n",
       "      <td>-0.809644</td>\n",
       "      <td>0.199603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>596 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age     MaxHR   Oldpeak  ChestPainType_ASY  ChestPainType_ATA  \\\n",
       "0    0.475397 -0.534464  0.296557                1.0                0.0   \n",
       "1   -0.794222  0.526945 -0.866898                0.0                0.0   \n",
       "2   -0.053611 -0.180661 -0.866898                0.0                1.0   \n",
       "3   -1.217428 -0.455841  0.878285                1.0                0.0   \n",
       "4    1.110206 -1.595873  1.266103                1.0                0.0   \n",
       "..        ...       ...       ...                ...                ...   \n",
       "591 -1.852237  1.706288 -0.866898                0.0                1.0   \n",
       "592 -1.534832 -1.202758 -0.866898                0.0                1.0   \n",
       "593 -1.005825  0.291076  1.847831                1.0                0.0   \n",
       "594  0.475397 -1.045513  1.072194                0.0                1.0   \n",
       "595  2.379825 -0.809644  0.199603                0.0                0.0   \n",
       "\n",
       "     ChestPainType_NAP  ChestPainType_TA  Sex_F  Sex_M  ExerciseAngina_N  \\\n",
       "0                  0.0               0.0    0.0    1.0               0.0   \n",
       "1                  1.0               0.0    0.0    1.0               1.0   \n",
       "2                  0.0               0.0    0.0    1.0               1.0   \n",
       "3                  0.0               0.0    0.0    1.0               0.0   \n",
       "4                  0.0               0.0    0.0    1.0               0.0   \n",
       "..                 ...               ...    ...    ...               ...   \n",
       "591                0.0               0.0    0.0    1.0               1.0   \n",
       "592                0.0               0.0    0.0    1.0               1.0   \n",
       "593                0.0               0.0    0.0    1.0               0.0   \n",
       "594                0.0               0.0    0.0    1.0               0.0   \n",
       "595                1.0               0.0    1.0    0.0               1.0   \n",
       "\n",
       "     ExerciseAngina_Y  RestingECG_LVH  RestingECG_Normal  RestingECG_ST  \\\n",
       "0                 1.0             0.0                1.0            0.0   \n",
       "1                 0.0             0.0                1.0            0.0   \n",
       "2                 0.0             0.0                1.0            0.0   \n",
       "3                 1.0             0.0                1.0            0.0   \n",
       "4                 1.0             1.0                0.0            0.0   \n",
       "..                ...             ...                ...            ...   \n",
       "591               0.0             0.0                1.0            0.0   \n",
       "592               0.0             0.0                1.0            0.0   \n",
       "593               1.0             0.0                1.0            0.0   \n",
       "594               1.0             0.0                1.0            0.0   \n",
       "595               0.0             0.0                0.0            1.0   \n",
       "\n",
       "     ST_Slope  \n",
       "0         1.0  \n",
       "1         2.0  \n",
       "2         2.0  \n",
       "3         1.0  \n",
       "4         0.0  \n",
       "..        ...  \n",
       "591       2.0  \n",
       "592       2.0  \n",
       "593       0.0  \n",
       "594       1.0  \n",
       "595       1.0  \n",
       "\n",
       "[596 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df = concatenate_features(scaled_df, encoded_df)\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "165e1c04-25b3-4158-a08a-0063708b7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    y_train: pd.Series, \n",
    "    model_path: str, \n",
    "    scaler_path: str,\n",
    "    OH_encoder_path: str,\n",
    "    OR_encoder_path:str):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(transformed_df, y_train)\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(one_hot_encoder, OH_encoder_path)\n",
    "    joblib.dump(ordinal_encoder, OR_encoder_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c16b6c8-7a10-48b9-9244-9086706b4047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOHAM\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"C:\\Users\\SOHAM\\Git_Repositories\\DataScience_Projects\\dsp-heart-failure-prediction\\models\\log_model.joblib\"\n",
    "scaler_path = r\"C:\\Users\\SOHAM\\Git_Repositories\\DataScience_Projects\\dsp-heart-failure-prediction\\models\\scaler_HD.joblib\"\n",
    "OH_encoder_path = r\"C:\\Users\\SOHAM\\Git_Repositories\\DataScience_Projects\\dsp-heart-failure-prediction\\models\\one_hot_encoder_HD.joblib\"\n",
    "OR_encoder_path = r\"C:\\Users\\SOHAM\\Git_Repositories\\DataScience_Projects\\dsp-heart-failure-prediction\\models\\ordinal_encoder_HD.joblib\"\n",
    "model = train_model(transformed_df, y_train, model_path, scaler_path, OH_encoder_path, OR_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a1b137-65d6-4a41-a756-81b972dce05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_testing_set(X_test: pd.DataFrame, Nominal_features: list, Ordinal_features: list, continuos_features: list, OH_encoder, OR_encoder, scaler) -> pd.DataFrame:\n",
    "    # Encode categorical features\n",
    "    \n",
    "    OH_encoder = joblib.load(OH_encoder)\n",
    "    OR_encoder = joblib.load(OR_encoder)\n",
    "    scaler = joblib.load(scaler)\n",
    "    Ordinal_df = X_test[Ordinal_features]\n",
    "    Nominal_df = X_test[Nominal_features]\n",
    "    continuous_df = X_test[continuos_features]\n",
    "\n",
    "    \n",
    "    encoded_oh_features = OH_encoder.transform(Nominal_df)\n",
    "    encoded_or_features = OR_encoder.transform(Ordinal_df)\n",
    "    oh_encoded_df = pd.DataFrame(encoded_oh_features, columns=OH_encoder.get_feature_names_out(Nominal_features))\n",
    "    or_encoded_df = pd.DataFrame(encoded_or_features, columns=Ordinal_features)\n",
    "    encoded_df = pd.concat([oh_encoded_df, or_encoded_df], axis =1)\n",
    "    \n",
    "    # Scale continuous features\n",
    "    scaled_features = scaler.transform(continuous_df)\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=continuos_features)\n",
    "    \n",
    "    # Concatenate features\n",
    "    processed_test_df = pd.concat([scaled_df, encoded_df], axis =1)\n",
    "    y_pred = model.predict(processed_test_df)\n",
    "    return processed_test_df,y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fdb7855-4683-4b29-8821-ef57b486e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set, y_pred = process_testing_set(X_test, Nominal_features, Ordinal_features, continuos_features, OH_encoder_path, OR_encoder_path,\n",
    "                                          scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f12de55-3cdf-4c14-b650-acae350470fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test: pd.Series, y_pred: np.ndarray) -> dict[str, str]:\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    return {'Accuracy score is': str(score)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c4e4090-20d6-43aa-806f-19c3f95d4583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy score is': '0.8416149068322981'}\n"
     ]
    }
   ],
   "source": [
    "evaluate = evaluate_model(y_test, y_pred)\n",
    "print(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d77a6-2926-4996-ad86-3916644b3f84",
   "metadata": {},
   "source": [
    "# **MODEL INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61fdcc03-72bb-47f2-b4fa-e7cd8449beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_transformers(model_path: str, scaler_path: str, OH_encoder_path: str, OR_encoder_path: str):\n",
    "    \"\"\"\n",
    "    Loads the pre-trained model, scaler, and encoder.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved model.\n",
    "        scaler_path (str): Path to the saved scaler.\n",
    "        encoder_path (str): Path to the saved encoder.\n",
    "\n",
    "    Returns:\n",
    "        model: Loaded model object.\n",
    "        scaler: Loaded scaler object.\n",
    "        encoder: Loaded encoder object.\n",
    "    \"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    one_hot_encoder = joblib.load(OH_encoder_path)\n",
    "    ordinal_encoder = joblib.load(OR_encoder_path)\n",
    "    return model, scaler, one_hot_encoder, ordinal_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b17609c2-c706-488e-b0d2-193801137b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, scaler, one_hot_encoder, ordinal_encoder = load_model_and_transformers(model_path, scaler_path, OH_encoder_path, OR_encoder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14929233-b8f2-406a-b2c5-8b2abd7267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_and_predict(Testing_df: pd.DataFrame, scaler, one_hot_encoder, ordinal_encoder, model, continuos_features: list, Nominal_features: list, Ordinal_features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the input data by scaling continuous features and encoding categorical features.\n",
    "\n",
    "    Args:\n",
    "        input_data (pd.DataFrame): The data to preprocess.\n",
    "        scaler: Scaler object to scale continuous features.\n",
    "        encoder: Encoder object to encode categorical features.\n",
    "        continuous_features (list): List of continuous feature names.\n",
    "        discrete_features (list): List of discrete feature names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed data.\n",
    "    \"\"\"\n",
    "    test_scaled = scaler.transform(Testing_df[continuos_features])\n",
    "    test_one_hot_encoded = one_hot_encoder.transform(Testing_df[Nominal_features])\n",
    "    test_ordinal_encoded = ordinal_encoder.transform(Testing_df[Ordinal_features])\n",
    "    \n",
    "\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=continuos_features)\n",
    "    test_one_hot_encoded_df = pd.DataFrame(test_one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(Nominal_features))\n",
    "    test_ordinal_encoded_df = pd.DataFrame(test_ordinal_encoded, columns=Ordinal_features)\n",
    "\n",
    "    transformed_encoded_df = pd.concat([test_one_hot_encoded_df, test_ordinal_encoded_df], axis=1)\n",
    "    transformed_test_df = pd.concat([test_scaled_df, transformed_encoded_df], axis=1)\n",
    "    predict_heart_patients = model.predict(transformed_test_df)\n",
    "    return predict_heart_patients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be7c30da-055a-4658-84f9-6bcda9a651d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0\n",
      " 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0\n",
      " 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1\n",
      " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
      " 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
      " 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1\n",
      " 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
      " 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "Test_dataset = r\"C:\\Users\\SOHAM\\Git_Repositories\\DataScience_Projects\\dsp-heart-failure-prediction\\data\\heart.csv\"\n",
    "Testing_df = pd.read_csv(Test_dataset)\n",
    "predict_heart_patients = preprocess_data_and_predict(Testing_df, scaler, one_hot_encoder, ordinal_encoder, model ,continuos_features, Nominal_features, Ordinal_features)\n",
    "print(predict_heart_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f03dd-9e64-4982-9899-23fe9ee059f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
